Here, we are using two algorithms: 
1.Random forest 
2.Suppoet vector machine 
After checking the accuracy level of both the algorithms we proceed with the one with the better  
accuracy rate.

1.Random Forest Regression: 

#importing the libraries 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
#importing the dataset 
dataset = pd.read_csv("Dataset.csv") 
from sklearn.preprocessing import LabelEncoder  
le = LabelEncoder()  
dataset['Date']= le.fit_transform(dataset['Date']) 
X = dataset.iloc[:, 2:6].values 
y = dataset.iloc[:, 6:7].values 
#splitting into Train and Test data 
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) 
#Featuring Scaling 
from sklearn.preprocessing import StandardScaler 
sc_X = StandardScaler() 

X_train = sc_X.fit_transform(X_train) 
X_test = sc_X.fit_transform(X_test) 
#Fitting the Random Forest Regression 
from sklearn.ensemble import RandomForestRegressor 
regressor = RandomForestRegressor(n_estimators = 300, random_state = 0) 
regressor.fit(X_train,y_train) 
#Predicting the Results 
y_pred = regressor.predict(X_test) 
if(y_pred.all()<2.5): 
y_pred=np.round(y_pred-0.5) 
else: 
y_pred=np.round(y_pred+0.5) 
#Calculationg the error and accuracy 
df1=(y_pred-y_test)/y_test 
df1=round(df1.mean()*100,2) 
print("Error = ",df1,"%")  
a=100-df1 
print("Accuracy= ",a,"%") 

2.Support vector Machine: 

#importing the libraries 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
#importing the dataset 
dataset = pd.read_csv("Dataset.csv") 
from sklearn.preprocessing import LabelEncoder  
le = LabelEncoder()  
dataset['Date']= le.fit_transform(dataset['Date']) 
X = dataset.iloc[:, 1:6].values 
y = dataset.iloc[:, 6:7].values 
#splitting into Train and Test data 
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) 
#Featuring Scaling 
from sklearn.preprocessing import StandardScaler 
sc_X = StandardScaler() 
X_train = sc_X.fit_transform(X_train) 
X_test = sc_X.fit_transform(X_test) 
#fitting SVR to the dataset 
from sklearn.svm import SVR 
regressor = SVR(kernel = 'rbf') 
regressor.fit(X_train,y_train) 

#Predicting the Test Set Results 
y_pred = regressor.predict(X_test) 
if(y_pred.all()<2.5): 
y_pred=np.round(y_pred-0.5)else: 
y_pred=np.round(y_pred+0.5) 
#Calculationg the error and accuracy 
df1=(y_pred-y_test)/y_test 
df1=round(df1.mean()*100,2) 
print("Error = ",df1,"%")  
a=100-df1 
print("Accuracy= ",a,"%")

3.Feature importance: 

In this we find the importance of each feature in the dataset and based on that we can remove the 
features with least importance. 
Code: 
import pandas as pd 
import numpy as np 
from sklearn.preprocessing import LabelEncoder 
from sklearn.preprocessing import StandardScaler 
from sklearn.ensemble import RandomForestRegressor 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import r2_score 
# Load the dataset 
dataset = pd.read_csv("Dataset.csv") 
le = LabelEncoder() 
dataset['Date'] = le.fit_transform(dataset['Date']) 
dataset['Day'] = le.fit_transform(dataset['Day']) 
# Selecting input features and target variable 
X = dataset[['Day', 'CodedDay', 'Zone', 'Weather', 'Temperature']].values 
y = dataset['Traffic'].values 
# Splitting into Train and Test data 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) 
# Featuring Scaling 
sc_X = StandardScaler() 
X_train = sc_X.fit_transform(X_train) 
X_test = sc_X.transform(X_test) 
# Fitting the Random Forest Regression 

regressor = RandomForestRegressor(n_estimators=300, random_state=0) 
regressor.fit(X_train, y_train)                                                                 
# Print feature importance 
feature_importance = regressor.feature_importances_ 
# Create a DataFrame to display the feature importance scores 
feature_importance_df = pd.DataFrame({'Feature': ['Day', 'CodedDay', 'Zone', 'Weather', 
'Temperature'], 'Importance': feature_importance}) 
# Sort the DataFrame by importance in descending order 
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False) 
# Print the sorted feature importance 
print("Feature Importance:") 
print(feature_importance_df) 

4.Creating a web application using streamlit: 


import streamlit as st 
import pandas as pd 
import numpy as np 
from sklearn.preprocessing import LabelEncoder, StandardScaler 
from sklearn.ensemble import RandomForestRegressor 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score 
# Load the dataset 
dataset = pd.read_csv(r"C:\Users\G MANISH\Downloads\Dataset.csv") 
le = LabelEncoder() 
dataset['Date'] = le.fit_transform(dataset['Date']) 
dataset['Day'] = le.fit_transform(dataset['Day']) 
# Create a dictionary mapping day names to their numerical values 
day_mapping = {day_name: encoded_day + 1 for encoded_day, day_name in 
enumerate(le.classes_)} 
# Create a dictionary mapping zone names to their numerical values 
zone_mapping = {chr(ord('A') + i): i + 1 for i in range(9)} 
# Create a dictionary mapping output labels to their numerical values 
output_mapping = {1: 'Less Traffic', 2: 'Medium Traffic', 3: 'High Traffic', 4: 'Very High Traffic', 5: 

'Extremely High Traffic'} 
# Selecting input features and target variable 
X = dataset[['Day', 'CodedDay', 'Zone', 'Weather', 'Temperature']].values 
y = dataset['Traffic'].values 
# Splitting into Train and Test data 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) 
# Featuring Scaling 
sc_X = StandardScaler() 
X_train = sc_X.fit_transform(X_train) 
X_test = sc_X.transform(X_test) 
# Fitting the Random Forest Regression 
regressor = RandomForestRegressor(n_estimators=300, random_state=0) 
regressor.fit(X_train, y_train) 
def predict_traffic(day, zone, weather, temperature): 
# Encoding the user input for 'Day' and 'Zone' 
day_encoded = day_mapping[day] 
zone_encoded = zone_mapping[zone] 
# Creating a numpy array with user input 
user_input = np.array([[day_encoded, 0, zone_encoded, weather, temperature]]) 

# Scaling the user input 
user_input_scaled = sc_X.transform(user_input) 
# Predicting the result for user input 
user_prediction = regressor.predict(user_input_scaled) 
# Rounding the prediction to the nearest integer 
user_prediction_rounded = np.round(user_prediction) 
# Mapping the numeric prediction to the output labels 
output_label = output_mapping.get(int(user_prediction_rounded[0]), 'Unknown') 
return output_label 
# Streamlit app 
def main(): 
st.title("Traffic Prediction App") 
# Selectbox for day 
selected_day = st.selectbox("Select the day:", list(day_mapping.keys())) 
# Selectbox for zone 
selected_zone = st.selectbox("Select the zone:", list(zone_mapping.keys())) 
weather = st.number_input("Enter the weather:", value=0) 
temperature = st.number_input("Enter the temperature:") 

# Make prediction 
if st.button("Predict Traffic"): 
prediction = predict_traffic(selected_day, selected_zone, weather, temperature) 
st.success(f"Predicted Traffic: {prediction}") 
# Evaluate the model on the test set 
y_pred = regressor.predict(X_test) 
# Calculate metrics 
mae = mean_absolute_error(y_test, y_pred) 
mse = mean_squared_error(y_test, y_pred) 
r2 = r2_score(y_test, y_pred) 
st.info(f"Mean Absolute Error: {mae:.2f}") 
st.info(f"Mean Squared Error: {mse:.2f}") 
st.info(f"R-squared: {r2:.2f}") 
if __name__ == "__main__": 
main()
